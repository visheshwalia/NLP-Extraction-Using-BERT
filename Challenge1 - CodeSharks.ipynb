{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62746fa9-ec9d-498b-b6a6-67bf0a98c71e",
   "metadata": {},
   "source": [
    "# BERT Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a877028b-625d-4309-9256-fb38a1c4cec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1ad3e7548b645e4a82396464cf77c1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e172c79ea404c49a4bc779bf961764c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "750a4bcb7be84da484d31ea92b29ca20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6850007cf0d4ff19b07a65c84f2011e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d14a64f68304ec8a83893b07dd823c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 4.14.326, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 01:38, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.673767</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.687200</td>\n",
       "      <td>0.603976</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833043</td>\n",
       "      <td>0.835664</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.687200</td>\n",
       "      <td>0.511473</td>\n",
       "      <td>0.854167</td>\n",
       "      <td>0.853595</td>\n",
       "      <td>0.859788</td>\n",
       "      <td>0.854167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for fold 1:\n",
      "Training Results:\n",
      "Accuracy: 0.9166666666666666\n",
      "F1: 0.9160839160839161\n",
      "Precision: 0.9285714285714285\n",
      "Recall: 0.9166666666666666\n",
      "Confusion Matrix for Training Data on Fold 1:\n",
      "[[20  4]\n",
      " [ 0 24]]\n",
      "Validation Results:\n",
      "Accuracy: 0.8541666666666666\n",
      "F1: 0.85359477124183\n",
      "Precision: 0.8597883597883597\n",
      "Recall: 0.8541666666666666\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.14.326, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Validation Data on Fold 1:\n",
      "[[19  5]\n",
      " [ 2 22]]\n",
      "Training on fold 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 01:31, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.450366</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.937255</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.516400</td>\n",
       "      <td>0.401578</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.916084</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.516400</td>\n",
       "      <td>0.307943</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.979158</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.979167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for fold 2:\n",
      "Training Results:\n",
      "Accuracy: 0.9583333333333334\n",
      "F1: 0.9582608695652176\n",
      "Precision: 0.9615384615384616\n",
      "Recall: 0.9583333333333334\n",
      "Confusion Matrix for Training Data on Fold 2:\n",
      "[[24  0]\n",
      " [ 2 22]]\n",
      "Validation Results:\n",
      "Accuracy: 0.9791666666666666\n",
      "F1: 0.9791576204950064\n",
      "Precision: 0.98\n",
      "Recall: 0.9791666666666666\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Validation Data on Fold 2:\n",
      "[[24  0]\n",
      " [ 1 23]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import re\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Function to set seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Set the seed\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "# Ensure determinism in PyTorch\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Function to clean text by removing punctuations\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# Function to tokenize text\n",
    "def tokenize_function(text):\n",
    "    return tokenizer(text, padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# Function to compute metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=-1)\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    precision = precision_score(labels, preds, average='weighted')\n",
    "    recall = recall_score(labels, preds, average='weighted')\n",
    "    \n",
    "    # Returning metrics as a dictionary\n",
    "    return {\n",
    "        'accuracy': accuracy, \n",
    "        'f1': f1, \n",
    "        'precision': precision, \n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/data/Bert_Model_Train.csv'\n",
    "data = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "\n",
    "# Clean and tokenize the Context column\n",
    "data['Context'] = data['Context'].apply(clean_text)\n",
    "data['tokenized'] = data.apply(lambda row: tokenize_function(row['Name'] + \" \" + row['Context']), axis=1)\n",
    "\n",
    "# Prepare data for oversampling\n",
    "data['input_ids'] = data['tokenized'].apply(lambda x: x['input_ids'])\n",
    "data['attention_mask'] = data['tokenized'].apply(lambda x: x['attention_mask'])\n",
    "\n",
    "# Map labels\n",
    "label_mapping = {'BCRP_substrate': 0, 'BCRP_inhibitor': 1}\n",
    "data['Activity_encoded'] = data['Activity'].map(label_mapping)\n",
    "\n",
    "# Define dataset class\n",
    "class DrugDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_ids, attention_mask, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(self.attention_mask[idx], dtype=torch.long),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Load the BERT model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=20,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    learning_rate=2e-5,\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Apply cross-validation\n",
    "skf = StratifiedKFold(n_splits=2, shuffle=True, random_state=SEED)\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(data, data['Activity_encoded'])):\n",
    "    train_data = data.iloc[train_idx]\n",
    "    val_data = data.iloc[val_idx]\n",
    "\n",
    "    # Oversampling with SMOTE\n",
    "    smote = SMOTE(random_state=SEED)\n",
    "    train_resampled, train_labels = smote.fit_resample(\n",
    "        np.hstack((np.array(train_data['input_ids'].tolist()), np.array(train_data['attention_mask'].tolist()))),\n",
    "        train_data['Activity_encoded']\n",
    "    )\n",
    "\n",
    "    val_resampled, val_labels = smote.fit_resample(\n",
    "        np.hstack((np.array(val_data['input_ids'].tolist()), np.array(val_data['attention_mask'].tolist()))),\n",
    "        val_data['Activity_encoded']\n",
    "    )\n",
    "\n",
    "    # Split input_ids and attention_mask after oversampling\n",
    "    train_input_ids = train_resampled[:, :512]\n",
    "    train_attention_mask = train_resampled[:, 512:]\n",
    "    val_input_ids = val_resampled[:, :512]\n",
    "    val_attention_mask = val_resampled[:, 512:]\n",
    "\n",
    "    # Convert to DrugDataset format\n",
    "    train_dataset = DrugDataset(train_input_ids, train_attention_mask, train_labels)\n",
    "    val_dataset = DrugDataset(val_input_ids, val_attention_mask, val_labels)\n",
    "\n",
    "    # Initialize the Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics)\n",
    "\n",
    "    # Train the model\n",
    "    print(f\"Training on fold {fold+1}\")\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate the model\n",
    "    train_results = trainer.evaluate(train_dataset)\n",
    "    val_results = trainer.evaluate(val_dataset)\n",
    "\n",
    "    # Print training results\n",
    "    print(f\"Results for fold {fold+1}:\")\n",
    "    print(\"Training Results:\")\n",
    "    print(f\"Accuracy: {train_results['eval_accuracy']}\")\n",
    "    print(f\"F1: {train_results['eval_f1']}\")\n",
    "    print(f\"Precision: {train_results['eval_precision']}\")\n",
    "    print(f\"Recall: {train_results['eval_recall']}\")\n",
    "\n",
    "    train_predictions = trainer.predict(train_dataset)\n",
    "    train_preds = np.argmax(train_predictions.predictions, axis=-1)\n",
    "    train_labels = train_predictions.label_ids\n",
    "    \n",
    "    # Compute and print confusion matrices\n",
    "    train_conf_matrix = confusion_matrix(train_labels, train_preds)\n",
    "    print(f\"Confusion Matrix for Training Data on Fold {fold+1}:\")\n",
    "    print(train_conf_matrix)\n",
    "\n",
    "    # Print Validation results\n",
    "    print(\"Validation Results:\")\n",
    "    print(f\"Accuracy: {val_results['eval_accuracy']}\")\n",
    "    print(f\"F1: {val_results['eval_f1']}\")\n",
    "    print(f\"Precision: {val_results['eval_precision']}\")\n",
    "    print(f\"Recall: {val_results['eval_recall']}\")\n",
    "    \n",
    "    # Prediction on validation data\n",
    "    val_predictions = trainer.predict(val_dataset)\n",
    "    val_preds = np.argmax(val_predictions.predictions, axis=-1)\n",
    "    val_labels = val_predictions.label_ids\n",
    "    \n",
    "    # Compute and print confusion matrices\n",
    "    val_conf_matrix = confusion_matrix(val_labels, val_preds)\n",
    "    \n",
    "    print(f\"Confusion Matrix for Validation Data on Fold {fold+1}:\")\n",
    "    print(val_conf_matrix)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79a3c30-f167-4bea-96f6-0620acb3fd8c",
   "metadata": {},
   "source": [
    "# API and NLP Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfb91d4b-9df7-413d-a973-151464df5465",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from Bio import Entrez\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import warnings\n",
    "from drug_named_entity_recognition import find_drugs\n",
    "import collections\n",
    "import string\n",
    "import time\n",
    "\n",
    "# Function to fetch Canonical SMILES and Chemical ID for a given drug name from PubChem\n",
    "def get_canonical_smiles_and_chem_id(drug_name):\n",
    "    base_url = \"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/name\"\n",
    "    # Fetch Canonical SMILES\n",
    "    smiles_response = requests.get(f\"{base_url}/{drug_name}/property/CanonicalSMILES/JSON\")\n",
    "    if smiles_response.status_code == 200:\n",
    "        canonical_smiles = smiles_response.json()['PropertyTable']['Properties'][0]['CanonicalSMILES']\n",
    "    else:        \n",
    "        canonical_smiles = None\n",
    "        pass\n",
    "    time.sleep(0.2)\n",
    "    # Fetch Chemical ID\n",
    "    cid_response = requests.get(f\"{base_url}/{drug_name}/cids/JSON\")\n",
    "    if cid_response.status_code == 200:\n",
    "        chem_id = cid_response.json()['IdentifierList']['CID'][0]\n",
    "    else:\n",
    "        chem_id = None\n",
    "        pass\n",
    "\n",
    "    return canonical_smiles, chem_id\n",
    "\n",
    "# Function to search PubMed Central with a given query\n",
    "def search_pubmed_central(query, max_results=1000):\n",
    "    Entrez.email = \"vishesh.walia@outlook.com\"  # Set your email here\n",
    "    terms = query.split()\n",
    "    and_query = \" AND \".join([f'\"{term}\"[Title]' for term in terms])\n",
    "    handle = Entrez.esearch(db=\"pmc\", term=and_query, retmax=max_results)\n",
    "    record = Entrez.read(handle)\n",
    "    handle.close()\n",
    "    pmc_ids = record['IdList']\n",
    "    return pmc_ids\n",
    "\n",
    "# Function to predict the class label using the model\n",
    "def predict_with_model(model, tokenizer, drug_names, contexts):\n",
    "    # Concatenate drug_name and context for each pair\n",
    "    combined_texts = [drug_name + \" \" + context for drug_name, context in zip(drug_names, contexts)]\n",
    "\n",
    "    # Tokenize the combined input\n",
    "    encodings = tokenizer(combined_texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    \n",
    "    # Predict with the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encodings)\n",
    "    \n",
    "    # Convert logits to probabilities and get predicted class labels\n",
    "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    predictions = torch.argmax(probabilities, dim=1)\n",
    "    return predictions\n",
    "\n",
    "# Function to fetch and process an article from PubMed Central\n",
    "def fetch_and_process_article(pmc_id, cache, nlp):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    url = f\"https://www.ncbi.nlm.nih.gov/pmc/articles/{pmc_id}/?report=printable\"\n",
    "    time.sleep(1/3)\n",
    "    with requests.get(url, headers=headers) as response:\n",
    "        if response.status_code == 200:\n",
    "            html_content = response.text\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "            text = soup.get_text()\n",
    "            title_tag = soup.find('h1')\n",
    "            article_title = title_tag.get_text().strip() if title_tag else \"No Title Found\"\n",
    "\n",
    "            # Preprocess the context by removing punctuation and leading/trailing spaces\n",
    "            doc = nlp(text)\n",
    "            context = \" \".join([token.text.strip(string.punctuation) for token in doc])\n",
    "\n",
    "            # Identify drug names within the text\n",
    "            drugs_info = find_drugs(context.split(), is_ignore_case=True)\n",
    "\n",
    "            keywords = [\"inhibitor\", \"transport inhibitor\", \"inhibits\", \"non transporter\", \n",
    "                        \"unable to transport\", \"bcrp\", \"non inhibitor\", \"substrate\", \n",
    "                        \"transport substrate\", \"transporter\", 'substrates']\n",
    "            \n",
    "            unique_drug_context_pairs = set()\n",
    "            article_drugs = []\n",
    "\n",
    "            # Extract relevant drug-context pairs and fetch SMILES and ChemID\n",
    "            for drug, start, end in drugs_info:\n",
    "                drug_pos = context.lower().find(drug['name'].lower())\n",
    "                if drug_pos != -1:\n",
    "                    start_pos = max(drug_pos - 300, 0)\n",
    "                    end_pos = min(drug_pos + 300, len(context))\n",
    "                    drug_context = context[start_pos:end_pos].lower()\n",
    "\n",
    "                    if any(keyword in drug_context for keyword in keywords):\n",
    "                        drug_context_key = (drug['name'].lower(), drug_context)\n",
    "                        if drug_context_key not in unique_drug_context_pairs:\n",
    "                            unique_drug_context_pairs.add(drug_context_key)\n",
    "                            canonical_smiles, chem_id = cache.get(drug['name'], get_canonical_smiles_and_chem_id(drug['name']))\n",
    "                            cache[drug['name']] = (canonical_smiles, chem_id)\n",
    "                            article_drugs.append({\n",
    "                                'PMC ID': pmc_id, \n",
    "                                'Drug Name': drug['name'], \n",
    "                                'Context': drug_context, \n",
    "                                'Canonical SMILES': canonical_smiles, \n",
    "                                'ChemID': chem_id\n",
    "                            })\n",
    "\n",
    "            return article_title, article_drugs\n",
    "        else:\n",
    "            print(f\"Failed to fetch article for PMC ID {pmc_id}. Status code: {response.status_code}\")\n",
    "            return \"\", []\n",
    "\n",
    "\n",
    "\n",
    "# Initialize Spacy model outside of the function for efficiency\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "\n",
    "def main(search_queries):\n",
    "    max_results = 1000\n",
    "    all_drugs = []\n",
    "    all_references = []\n",
    "    processed_pmc_ids = set()  # Set to track processed PMC IDs\n",
    "    cache = {}  # Cache for storing already fetched SMILES and ChemIDs\n",
    "    nlp = spacy.blank(\"en\")  # Load Spacy language model\n",
    "    warnings.filterwarnings('ignore')  # Optional to suppress warnings\n",
    "\n",
    "    for query in search_queries:\n",
    "        pmc_ids = search_pubmed_central(query, max_results)\n",
    "        for pmc_id in pmc_ids:\n",
    "            if pmc_id not in processed_pmc_ids:\n",
    "                article_title, article_drugs = fetch_and_process_article(pmc_id, cache, nlp)\n",
    "                all_drugs.extend(article_drugs)  # Add unique drug entries to all_drugs list\n",
    "                all_references.append({'Index': len(all_references) + 1, 'PMC ID': pmc_id, 'Article Title': article_title})\n",
    "                processed_pmc_ids.add(pmc_id)  # Mark the PMC ID as processed\n",
    "\n",
    "    df = pd.DataFrame(all_drugs)\n",
    "    references_df = pd.DataFrame(all_references)\n",
    "    references_dict = references_df.set_index('PMC ID')['Index'].to_dict()\n",
    "\n",
    "    #Assuming the existence of a model and tokenizer for prediction\n",
    "    predicted_labels = predict_with_model(model, tokenizer, df['Drug Name'].tolist(), df['Context'].tolist())\n",
    "    reverse_label_mapping = {0: 'BCRP_substrate', 1: 'BCRP_inhibitor'}\n",
    "    df['Predicted Label'] = [reverse_label_mapping[label.item()] for label in predicted_labels]\n",
    "    \n",
    "    #Group by Drug Name and determine the most frequent label\n",
    "    df_grouped = df.groupby(['Drug Name', 'Predicted Label']).size().reset_index(name='Count')\n",
    "    df_max_label = df_grouped.loc[df_grouped.groupby('Drug Name')['Count'].idxmax()]\n",
    "\n",
    "    #Aggregate PMC IDs, Canonical SMILES, and Chemical ID\n",
    "    df_agg = df.groupby('Drug Name').agg({\n",
    "        'PMC ID': lambda x: ', '.join(sorted(set(x))),\n",
    "        'Canonical SMILES': 'first',\n",
    "        'ChemID': 'first'\n",
    "    }).reset_index()\n",
    "\n",
    "    df_combined = df_max_label.merge(df_agg, on='Drug Name')\n",
    "\n",
    "    def pmc_to_ref_index(pmc_ids):\n",
    "        pmc_ids = pmc_ids.split(', ')\n",
    "        return [references_dict.get(pmc_id) for pmc_id in pmc_ids]\n",
    "\n",
    "    df_combined['Ref Indexes'] = df_combined['PMC ID'].apply(pmc_to_ref_index)\n",
    "    df_combined['Ref Indexes'] = df_combined['Ref Indexes'].apply(lambda x: [i for i in x if i is not None])\n",
    "\n",
    "    ref_index_columns = df_combined['Ref Indexes'].apply(pd.Series)\n",
    "    ref_index_columns = ref_index_columns.rename(columns=lambda x: 'Ref' + str(x + 1))\n",
    "\n",
    "    df_final = pd.concat([df_combined.drop(['Ref Indexes', 'PMC ID'], axis=1), ref_index_columns], axis=1)\n",
    "    df_final = df_final.drop(columns=['Count'])\n",
    "    ref_index_columns = [col for col in df_final.columns if col.startswith('Ref')]\n",
    "    df_final[ref_index_columns] = df_final[ref_index_columns].fillna(0).applymap(lambda x: int(x) if not pd.isna(x) else pd.NA)\n",
    "\n",
    "    return df_final, references_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    search_queries = [\"BCRP DRUG TRANSPORT\", \"ABCG2 DRUG TRANSPORT\", \"Drug Response ABCG2\", \"Drug Response BCRP\", \"BCRP Efflux Transport\", 'ABCG2 Efflux',\n",
    "                      \"Multidrug resistance ATP-binding cassette transporters\", 'Multidrug BCRP','Multidrug ABCG2', 'BCRP Efflux', 'BCRP Resistance']\n",
    "    df_final, references_df = main(search_queries)\n",
    "    references_df.to_csv('/results/references.tsv', sep = '\\t')\n",
    "\n",
    "    df_final_dedup = df_final.drop_duplicates(subset='Drug Name')\n",
    "\n",
    "    # Calculations for metrics_summary\n",
    "    team_name = \"Model Kombat\"\n",
    "    team_contact = [\"vxw220000@utdallas.edu\", \"axc220027@utdallas.edu\"]\n",
    "    num_samples = df_final_dedup['Drug Name'].nunique()\n",
    "    num_references = references_df['PMC ID'].nunique()\n",
    "    num_inhibitor = df_final_dedup[df_final_dedup['Predicted Label'] == 'BCRP_inhibitor'].shape[0]\n",
    "    num_substrate = df_final_dedup[df_final_dedup['Predicted Label'] == 'BCRP_substrate'].shape[0]\n",
    "    percent_minority = min(num_inhibitor, num_substrate) / num_samples if num_samples > 0 else 0\n",
    "    num_smiles = df_final_dedup[df_final_dedup['Canonical SMILES'].notna() & (df_final_dedup['Canonical SMILES'] != 'missing') & (df_final_dedup['Canonical SMILES'] != 'not available')].shape[0]\n",
    "    percent_smiles = num_smiles / num_samples if num_samples > 0 else 0\n",
    "\n",
    "    # Creating metrics_summary dataframe\n",
    "    metrics_summary = pd.DataFrame({\n",
    "        \"team_name\": [team_name],\n",
    "        \"team_contact\": [team_contact],\n",
    "        \"num_samples\": [num_samples],\n",
    "        \"num_references\": [num_references],\n",
    "        \"num_inhibitor\": [num_inhibitor],\n",
    "        \"num_substrate\": [num_substrate],\n",
    "        \"percent_minority\": [percent_minority],\n",
    "        \"num_smiles\": [num_smiles],\n",
    "        \"percent_smiles\": [percent_smiles]\n",
    "    })\n",
    "\n",
    "    metrics_summary.to_csv('/results/metrics_summary.tsv', sep = '\\t')\n",
    "\n",
    "    # Renaming and reordering columns for training_data\n",
    "    df_final.rename(columns={\n",
    "        'Drug Name': 'Name',\n",
    "        'Canonical SMILES': 'SMILES',\n",
    "        'Predicted Label': 'Activity',\n",
    "        'ChemID': 'PubChem_CID'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Assuming 'Ref Indexes' is a list of references for each drug\n",
    "    # Expand this list into separate columns (Ref1, Ref2, etc.)\n",
    "    df_final.to_csv('/results/training_data.tsv', sep = '\\t')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db86ab1-6bb4-4b15-a948-74fb4df1e3ef",
   "metadata": {},
   "source": [
    "# Metrics Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f35d49-d9b9-442c-936b-b68d8e554f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf6b655-bbdb-4083-8a4c-9cbe7e60d587",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97138069-7564-4d48-9491-9b9586766fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total values in validation data: 51\n",
      "Matching drug names in extracted data & validation data: 35\n",
      "Matching substrates/inhibitors in extracted data & validation data: 28\n",
      "Accuracy: 80.0% on validation set\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Extract relevant columns from training data for comparison\n",
    "file_path = '/data/Validation_Data.tsv'\n",
    "training_data = pd.read_csv(file_path, sep = '\\t')\n",
    "training_data_relevant = training_data[['Name', 'Activity']].rename(columns={'Activity': 'Activity_actual'})\n",
    "\n",
    "# Merge the two dataframes on the 'Name' column to find matching entries\n",
    "validation_df = pd.merge(df_final, training_data_relevant, on='Name', how='inner')\n",
    "\n",
    "# Calculate the number of matching predictions\n",
    "matching_predictions = validation_df[validation_df['Activity'] == validation_df['Activity_actual']].shape[0]\n",
    "\n",
    "# Total number of drug names in df_final that are also in the training data\n",
    "total_matching_drug_names = validation_df.shape[0]\n",
    "\n",
    "print(f'Total values in validation data: {training_data_relevant.shape[0]}')\n",
    "print(f'Matching drug names in extracted data & validation data: {total_matching_drug_names}')\n",
    "print(f'Matching substrates/inhibitors in extracted data & validation data: {matching_predictions}')\n",
    "print('Accuracy: ' + str(round(matching_predictions/total_matching_drug_names,2)*100) + '%' + ' on validation set')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
